models:
  nano:
    name: "nano"
    params: 124_000 # ~124K parameters

    # Model architecture
    architecture:
      n_layers: 2
      n_heads: 2
      d_model: 64
      d_ff: 256
      max_seq_len: 512
      dropout: 0.1
      vocab_size: 50257

    # Data configuration
    target_tokens_billions: 0.01 # 10M tokens to train on

    # Training hyperparameters
    learning_rate: 3e-4
    batch_size: 8
    gradient_accumulation_steps: 4 # Effective batch size = 8 Ã— 4 = 32

    # Learning rate schedule (percentages of total training steps)
    warmup_percent: 10 # Warmup for 10% of training
    lr_scheduler: "cosine"
    warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
    min_lr_ratio: 0.1 # End at 10% of peak LR

    optimizer: "adamw"
    weight_decay: 0.1
    max_grad_norm: 1.0

    # Logging and saving
    save_every: 500
    eval_every: 250
    log_every: 50

  # micro:
  #   name: "micro"
  #   params: 1_000_000  # ~1M parameters

  #   architecture:
  #     n_layers: 4
  #     n_heads: 4
  #     d_model: 128
  #     d_ff: 512
  #     max_seq_len: 512
  #     dropout: 0.1
  #     vocab_size: 50257

  #   target_tokens_billions: 0.1   # 100M tokens to train on

  #   learning_rate: 2e-4
  #   batch_size: 32
  #   max_steps: 20000
  #   warmup_steps: 1000
  #   lr_scheduler: "cosine"
  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   save_every: 1000
  #   eval_every: 500
  #   log_every: 100

  # tiny:
  #   name: "tiny"
  #   params: 10_000_000  # ~10M parameters

  #   architecture:
  #     n_layers: 8
  #     n_heads: 8
  #     d_model: 256
  #     d_ff: 1024
  #     max_seq_len: 1024
  #     dropout: 0.1
  #     vocab_size: 50257

  #   target_tokens_billions: 1.0   # 1B tokens to train on

  #   learning_rate: 1e-4
  #   batch_size: 16
  #   max_steps: 50000
  #   warmup_steps: 2000
  #   lr_scheduler: "cosine"
  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   save_every: 2000
  #   eval_every: 1000
  #   log_every: 200

  # small:
  #   name: "small"
  #   params: 100_000_000  # ~100M parameters

  #   architecture:
  #     n_layers: 12
  #     n_heads: 12
  #     d_model: 512
  #     d_ff: 2048
  #     max_seq_len: 1024
  #     dropout: 0.1
  #     vocab_size: 50257

  #   target_tokens_billions: 10.0  # 10B tokens to train on

  #   learning_rate: 5e-5
  #   batch_size: 8
  #   max_steps: 100000
  #   warmup_steps: 5000
  #   lr_scheduler: "cosine"
  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   save_every: 5000
  #   eval_every: 2500
  #   log_every: 500

  # medium:
  #   name: "medium"
  #   params: 500_000_000  # ~500M parameters

  #   architecture:
  #     n_layers: 24
  #     n_heads: 16
  #     d_model: 1024
  #     d_ff: 4096
  #     max_seq_len: 1024
  #     dropout: 0.1
  #     vocab_size: 50257

  #   target_tokens_billions: 50.0  # 50B tokens to train on

  #   learning_rate: 3e-5
  #   batch_size: 4
  #   max_steps: 200000
  #   warmup_steps: 10000
  #   lr_scheduler: "cosine"
  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   save_every: 10000
  #   eval_every: 5000
  #   log_every: 1000

  # large:
  #   name: "large"
  #   params: 1_000_000_000  # ~1B parameters

  #   architecture:
  #     n_layers: 48
  #     n_heads: 32
  #     d_model: 2048
  #     d_ff: 8192
  #     max_seq_len: 1024
  #     dropout: 0.1
  #     vocab_size: 50257

  #   target_tokens_billions: 200.0 # 200B tokens to train on

  #   learning_rate: 1e-5
  #   batch_size: 2
  #   max_steps: 500000
  #   warmup_steps: 20000
  #   lr_scheduler: "cosine"
  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   save_every: 20000
  #   eval_every: 10000
  #   log_every: 2000
# Configuration Philosophy:
# - All hyperparameters are model-specific and centralized here
# - target_tokens_billions: How much data each model will train on (with distributed sampling)
# - Dataset preprocessing amount is set in data.yaml (should be >= max target_tokens)
# - Scaling follows compute-optimal relationships
# - Learning rates decrease with model size
# - Training steps increase with model size
# - Logging frequency scales with training length
