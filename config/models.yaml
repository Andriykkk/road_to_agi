models:
  tiny:
    name: "tiny"
    params: 6_500_000 # ~6.5M actual parameters (including embeddings)

    # Model architecture
    architecture:
      n_layers: 2
      n_heads: 2
      d_model: 64
      d_ff: 256
      max_seq_len: 512
      dropout: 0.1
      vocab_size: 50257

    # Data configuration (Chinchilla: ~20 tokens per parameter)
    target_tokens_billions: 0.13 # 130M tokens (20x params)

    # Training hyperparameters
    learning_rate: 2e-4
    batch_size: 8
    gradient_accumulation_steps: 4 # Effective batch size = 8 × 4 = 32

    # Learning rate schedule (percentages of total training steps)
    warmup_percent: 10 # Warmup for 10% of training
    lr_scheduler: "cosine"
    warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
    min_lr_ratio: 0.1 # End at 10% of peak LR

    optimizer: "adamw"
    weight_decay: 0.1
    max_grad_norm: 1.0

    # Logging and saving
    save_every: 1000
    eval_every: 500

  small:
    name: "small"
    params: 13_000_000 # ~13M actual parameters (including embeddings)

    # Model architecture
    architecture:
      n_layers: 4
      n_heads: 4
      d_model: 128
      d_ff: 512
      max_seq_len: 512
      dropout: 0.1
      vocab_size: 50257

    # Data configuration (Chinchilla: ~20 tokens per parameter)
    target_tokens_billions: 0.27 # 270M tokens (20x params)

    # Training hyperparameters
    learning_rate: 1.5e-4
    batch_size: 8
    gradient_accumulation_steps: 4 # Effective batch size = 8 × 4 = 32

    # Learning rate schedule (percentages of total training steps)
    warmup_percent: 10 # Warmup for 10% of training
    lr_scheduler: "cosine"
    warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
    min_lr_ratio: 0.1 # End at 10% of peak LR

    optimizer: "adamw"
    weight_decay: 0.1
    max_grad_norm: 1.0

    # Logging and saving
    save_every: 1500
    eval_every: 750

  # medium:
  #   name: "medium"
  #   params: 22_000_000 # ~22M actual parameters (including embeddings)

  #   # Model architecture
  #   architecture:
  #     n_layers: 6
  #     n_heads: 6
  #     d_model: 192
  #     d_ff: 768
  #     max_seq_len: 512
  #     dropout: 0.1
  #     vocab_size: 50257

  #   # Data configuration (Chinchilla: ~20 tokens per parameter)
  #   target_tokens_billions: 0.44 # 440M tokens (20x params)

  #   # Training hyperparameters
  #   learning_rate: 1.2e-4
  #   batch_size: 6
  #   gradient_accumulation_steps: 6 # Effective batch size = 6 × 6 = 36

  #   # Learning rate schedule (percentages of total training steps)
  #   warmup_percent: 10 # Warmup for 10% of training
  #   lr_scheduler: "cosine"
  #   warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
  #   min_lr_ratio: 0.1 # End at 10% of peak LR

  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   # Logging and saving
  #   save_every: 2000
  #   eval_every: 1000

  # large:
  #   name: "large"
  #   params: 38_000_000 # ~38M actual parameters (including embeddings)

  #   # Model architecture
  #   architecture:
  #     n_layers: 8
  #     n_heads: 8
  #     d_model: 256
  #     d_ff: 1024
  #     max_seq_len: 512
  #     dropout: 0.1
  #     vocab_size: 50257

  #   # Data configuration (Chinchilla: ~20 tokens per parameter)
  #   target_tokens_billions: 0.76 # 760M tokens (20x params)

  #   # Training hyperparameters
  #   learning_rate: 1e-4
  #   batch_size: 4
  #   gradient_accumulation_steps: 8 # Effective batch size = 4 × 8 = 32

  #   # Learning rate schedule (percentages of total training steps)
  #   warmup_percent: 10 # Warmup for 10% of training
  #   lr_scheduler: "cosine"
  #   warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
  #   min_lr_ratio: 0.1 # End at 10% of peak LR

  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   # Logging and saving
  #   save_every: 3000
  #   eval_every: 1500

  # xlarge:
  #   name: "xlarge"
  #   params: 125_000_000 # ~125M actual parameters (≈ GPT-2 small)

  #   # Model architecture
  #   architecture:
  #     n_layers: 12
  #     n_heads: 12
  #     d_model: 768
  #     d_ff: 3072
  #     max_seq_len: 1024
  #     dropout: 0.1
  #     vocab_size: 50257

  #   # Data configuration (Chinchilla: ~20 tokens per parameter)
  #   target_tokens_billions: 2.5 # 2.5B tokens (20x params)

  #   # Training hyperparameters
  #   learning_rate: 6e-5
  #   batch_size: 2
  #   gradient_accumulation_steps: 16 # Effective batch size = 2 × 16 = 32

  #   # Learning rate schedule (percentages of total training steps)
  #   warmup_percent: 10 # Warmup for 10% of training
  #   lr_scheduler: "cosine"
  #   warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
  #   min_lr_ratio: 0.1 # End at 10% of peak LR

  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   # Logging and saving
  #   save_every: 10000
  #   eval_every: 5000

  # large:
  #   name: "large"
  #   params: 320_000_000 # ~320M parameters

  #   # Model architecture
  #   architecture:
  #     n_layers: 18
  #     n_heads: 16
  #     d_model: 1024
  #     d_ff: 4096
  #     max_seq_len: 1024
  #     dropout: 0.1
  #     vocab_size: 50257

  #   # Data configuration (Chinchilla: ~20 tokens per parameter)
  #   target_tokens_billions: 6.4 # 6.4B tokens (20x params)

  #   # Training hyperparameters
  #   learning_rate: 4e-5
  #   batch_size: 1
  #   gradient_accumulation_steps: 32 # Effective batch size = 1 × 32 = 32

  #   # Learning rate schedule (percentages of total training steps)
  #   warmup_percent: 10 # Warmup for 10% of training
  #   lr_scheduler: "cosine"
  #   warmup_start_lr_ratio: 0.1 # Start warmup at 10% of peak LR
  #   min_lr_ratio: 0.1 # End at 10% of peak LR

  #   optimizer: "adamw"
  #   weight_decay: 0.1
  #   max_grad_norm: 1.0

  #   # Logging and saving
  #   save_every: 20000
  #   eval_every: 10000
